{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "45o7GIBjhiWT",
    "outputId": "1bacd584-7c4c-45fe-a151-b8532e44f91a"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nKgiulabiIjm",
    "outputId": "dc53b7a3-8880-46fd-9e0b-5d800ac7ca4f"
   },
   "outputs": [],
   "source": [
    "! cp -av /content/drive/MyDrive/ThesisDataset/MedSet2/ ./dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LtQUAhO1iK_y"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from markdown import markdown\n",
    "from pathlib import Path\n",
    "pd.set_option('max_colwidth',300)\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POLzdoDCiQH9"
   },
   "source": [
    "## Dataset for PLBart (testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "KNBxAG3EiTGj",
    "outputId": "c5bef398-a27b-417f-a5ea-280095b039da"
   },
   "outputs": [],
   "source": [
    "#train_py = pd.read_json('./dataset/SmallDataset/python/training.jsonl',lines=True)\n",
    "#val_py = pd.read_json('./dataset/SmallDataset/python/validation.jsonl',lines=True)\n",
    "#test_py = pd.read_json('./dataset/SmallDataset/python/testing.jsonl',lines=True)\n",
    "\n",
    "train_java = pd.read_json('./dataset/java/training.jsonl',lines=True)\n",
    "val_java = pd.read_json('./dataset/java/validation.jsonl',lines=True)\n",
    "test_java = pd.read_json('./dataset/java/testing100.jsonl',lines=True)\n",
    "\"\"\"\n",
    "train_jscript = pd.read_json('./dataset/SmallDataset/javascript/training.jsonl',lines=True)\n",
    "val_jscript = pd.read_json('./dataset/SmallDataset/javascript/validation.jsonl',lines=True)\n",
    "test_jscript = pd.read_json('./dataset/SmallDataset/javascript/testing.jsonl',lines=True)\n",
    "\n",
    "train_php = pd.read_json('./dataset/SmallDataset/php/training.jsonl',lines=True)\n",
    "val_php = pd.read_json('./dataset/SmallDataset/php/validation.jsonl',lines=True)\n",
    "test_php = pd.read_json('./dataset/SmallDataset/php/testing.jsonl',lines=True)\n",
    "\n",
    "train_go = pd.read_json('./dataset/SmallDataset/go/training.jsonl',lines=True)\n",
    "val_go = pd.read_json('./dataset/SmallDataset/go/validation.jsonl',lines=True)\n",
    "test_go = pd.read_json('./dataset/SmallDataset/go/testing.jsonl',lines=True)\n",
    "\n",
    "train_ruby = pd.read_json('./dataset/SmallDataset/ruby/training.jsonl',lines=True)\n",
    "val_ruby = pd.read_json('./dataset/SmallDataset/ruby/validation.jsonl',lines=True)\n",
    "test_ruby = pd.read_json('./dataset/SmallDataset/ruby/testing.jsonl',lines=True)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "SU-ZeoAZ8F2q",
    "outputId": "0a4e9dda-1da9-43fc-9bb3-72a0babf4d91"
   },
   "outputs": [],
   "source": [
    "test_java['code2'] = test_java.apply(lambda row: (''.join(row['code_tokens'][1:-1]).replace('\\n',' ').replace(\"'\",'')),axis=1)\n",
    "test_java['code2'] = test_java.apply(lambda row: (''.join(row[\"code2\"].strip().split()).replace(',',' ')),axis=1)\n",
    "\n",
    "test_java['docstring2'] = test_java.apply(lambda row: (''.join(row['docstring_tokens'][1:-1]).replace('\\n',' ').replace(\"'\",'')),axis=1)\n",
    "test_java['docstring2'] = test_java.apply(lambda row: (''.join(row[\"docstring2\"].strip().split()).replace(',',' ')),axis=1)\n",
    "\n",
    "test_java.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtzk4Mfdi92h"
   },
   "source": [
    "## Common Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cDPff5Df4evk",
    "outputId": "3c60208c-a04a-4ed7-d0e4-37a91c184f1e"
   },
   "outputs": [],
   "source": [
    "! pip install transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ya1cqTpY_DNp",
    "outputId": "55aa3fdb-c79d-49dc-fbff-efb30db8f487"
   },
   "outputs": [],
   "source": [
    "! pip install tree_sitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5JuBvaVH4hkB",
    "outputId": "6e441431-66a8-4921-a9dc-96203c429f08"
   },
   "outputs": [],
   "source": [
    "! pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OzRKJK-Wia50"
   },
   "source": [
    "## GraphCodeBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a60a1J88isCQ"
   },
   "source": [
    "### Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRKSr9xuinDu"
   },
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation. \n",
    "# Licensed under the MIT license.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"\n",
    "        Build Seqence-to-Sequence.\n",
    "        \n",
    "        Parameters:\n",
    "\n",
    "        * `encoder`- encoder of seq2seq model. e.g. roberta\n",
    "        * `decoder`- decoder of seq2seq model. e.g. transformer\n",
    "        * `config`- configuration of encoder model. \n",
    "        * `beam_size`- beam size for beam search. \n",
    "        * `max_length`- max length of target for beam search. \n",
    "        * `sos_id`- start of symbol ids in target for beam search.\n",
    "        * `eos_id`- end of symbol ids in target for beam search. \n",
    "    \"\"\"\n",
    "    def __init__(self, encoder,decoder,config,beam_size=None,max_length=None,sos_id=None,eos_id=None):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder=decoder\n",
    "        self.config=config\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(2048, 2048)))\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        self.lsm = nn.LogSoftmax(dim=-1)\n",
    "        self.tie_weights()\n",
    "        \n",
    "        self.beam_size=beam_size\n",
    "        self.max_length=max_length\n",
    "        self.sos_id=sos_id\n",
    "        self.eos_id=eos_id\n",
    "        \n",
    "    def _tie_or_clone_weights(self, first_module, second_module):\n",
    "        \"\"\" Tie or clone module weights depending of weither we are using TorchScript or not\n",
    "        \"\"\"\n",
    "        if self.config.torchscript:\n",
    "            first_module.weight = nn.Parameter(second_module.weight.clone())\n",
    "        else:\n",
    "            first_module.weight = second_module.weight\n",
    "                  \n",
    "    def tie_weights(self):\n",
    "        \"\"\" Make sure we are sharing the input and output embeddings.\n",
    "            Export to TorchScript can't handle parameter sharing so we are cloning them instead.\n",
    "        \"\"\"\n",
    "        self._tie_or_clone_weights(self.lm_head,\n",
    "                                   self.encoder.embeddings.word_embeddings)        \n",
    "        \n",
    "    def forward(self, source_ids,source_mask,position_idx,attn_mask,target_ids=None,target_mask=None,args=None):   \n",
    "        #embedding\n",
    "        nodes_mask=position_idx.eq(0)\n",
    "        token_mask=position_idx.ge(2)        \n",
    "        inputs_embeddings=self.encoder.embeddings.word_embeddings(source_ids)\n",
    "        nodes_to_token_mask=nodes_mask[:,:,None]&token_mask[:,None,:]&attn_mask\n",
    "        nodes_to_token_mask=nodes_to_token_mask/(nodes_to_token_mask.sum(-1)+1e-10)[:,:,None]\n",
    "        avg_embeddings=torch.einsum(\"abc,acd->abd\",nodes_to_token_mask,inputs_embeddings)\n",
    "        inputs_embeddings=inputs_embeddings*(~nodes_mask)[:,:,None]+avg_embeddings*nodes_mask[:,:,None]  \n",
    "        \n",
    "        outputs = self.encoder(inputs_embeds=inputs_embeddings,attention_mask=attn_mask,position_ids=position_idx)\n",
    "        encoder_output = outputs[0].permute([1,0,2]).contiguous()\n",
    "        if target_ids is not None:  \n",
    "            attn_mask=-1e4 *(1-self.bias[:target_ids.shape[1],:target_ids.shape[1]])\n",
    "            tgt_embeddings = self.encoder.embeddings(target_ids).permute([1,0,2]).contiguous()\n",
    "            out = self.decoder(tgt_embeddings,encoder_output,tgt_mask=attn_mask,memory_key_padding_mask=(1-source_mask).bool())\n",
    "            hidden_states = torch.tanh(self.dense(out)).permute([1,0,2]).contiguous()\n",
    "            lm_logits = self.lm_head(hidden_states)\n",
    "            # Shift so that tokens < n predict n\n",
    "            active_loss = target_mask[..., 1:].ne(0).view(-1) == 1\n",
    "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = target_ids[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1))[active_loss],\n",
    "                            shift_labels.view(-1)[active_loss])\n",
    "\n",
    "            outputs = loss,loss*active_loss.sum(),active_loss.sum()\n",
    "            return outputs\n",
    "        else:\n",
    "            #Predict\n",
    "            my_logits=[] #new\n",
    "            preds=[]       \n",
    "            zero=torch.cuda.LongTensor(1).fill_(0)     \n",
    "            for i in range(source_ids.shape[0]):\n",
    "                context=encoder_output[:,i:i+1]\n",
    "                context_mask=source_mask[i:i+1,:]\n",
    "                beam = Beam(self.beam_size,self.sos_id,self.eos_id)\n",
    "                input_ids=beam.getCurrentState()\n",
    "                context=context.repeat(1, self.beam_size,1)\n",
    "                context_mask=context_mask.repeat(self.beam_size,1)\n",
    "                for _ in range(self.max_length): \n",
    "                    if beam.done():\n",
    "                        break\n",
    "                    attn_mask=-1e4 *(1-self.bias[:input_ids.shape[1],:input_ids.shape[1]])\n",
    "                    tgt_embeddings = self.encoder.embeddings(input_ids).permute([1,0,2]).contiguous()\n",
    "                    out = self.decoder(tgt_embeddings,context,tgt_mask=attn_mask,memory_key_padding_mask=(1-context_mask).bool())\n",
    "                    out = torch.tanh(self.dense(out))\n",
    "                    hidden_states=out.permute([1,0,2]).contiguous()[:,-1,:]\n",
    "                    out = self.lsm(self.lm_head(hidden_states)).data\n",
    "                    my_logits.append(out.clone()) #new  \n",
    "                    final_logits = torch.cat(my_logits,0)# new\n",
    "                    beam.advance(out)\n",
    "                    input_ids.data.copy_(input_ids.data.index_select(0, beam.getCurrentOrigin()))\n",
    "                    input_ids=torch.cat((input_ids,beam.getCurrentState()),-1)\n",
    "                hyp= beam.getHyp(beam.getFinal())\n",
    "                pred=beam.buildTargetTokens(hyp)[:self.beam_size]\n",
    "                pred=[torch.cat([x.view(-1) for x in p]+[zero]*(self.max_length-len(p))).view(1,-1) for p in pred]\n",
    "\n",
    "                preds.append(torch.cat(pred,0).unsqueeze(0))\n",
    "            \n",
    "            preds=torch.cat(preds,0)                \n",
    "            return final_logits,preds   \n",
    "        \n",
    "        \n",
    "\n",
    "class Beam(object):\n",
    "    def __init__(self, size,sos,eos):\n",
    "        self.size = size\n",
    "        self.tt = torch.cuda\n",
    "        # The score for each translation on the beam.\n",
    "        self.scores = self.tt.FloatTensor(size).zero_()\n",
    "        # The backpointers at each time-step.\n",
    "        self.prevKs = []\n",
    "        # The outputs at each time-step.\n",
    "        self.nextYs = [self.tt.LongTensor(size)\n",
    "                       .fill_(0)]\n",
    "        self.nextYs[0][0] = sos\n",
    "        # Has EOS topped the beam yet.\n",
    "        self._eos = eos\n",
    "        self.eosTop = False\n",
    "        # Time and k pair for finished.\n",
    "        self.finished = []\n",
    "\n",
    "    def getCurrentState(self):\n",
    "        \"Get the outputs for the current timestep.\"\n",
    "        batch = self.tt.LongTensor(self.nextYs[-1]).view(-1, 1)\n",
    "        return batch\n",
    "\n",
    "    def getCurrentOrigin(self):\n",
    "        \"Get the backpointers for the current timestep.\"\n",
    "        return self.prevKs[-1]\n",
    "\n",
    "    def advance(self, wordLk):\n",
    "        \"\"\"\n",
    "        Given prob over words for every last beam `wordLk` and attention\n",
    "        `attnOut`: Compute and update the beam search.\n",
    "\n",
    "        Parameters:\n",
    "\n",
    "        * `wordLk`- probs of advancing from the last step (K x words)\n",
    "        * `attnOut`- attention at the last step\n",
    "\n",
    "        Returns: True if beam search is complete.\n",
    "        \"\"\"\n",
    "        numWords = wordLk.size(1)\n",
    "\n",
    "        # Sum the previous scores.\n",
    "        if len(self.prevKs) > 0:\n",
    "            beamLk = wordLk + self.scores.unsqueeze(1).expand_as(wordLk)\n",
    "\n",
    "            # Don't let EOS have children.\n",
    "            for i in range(self.nextYs[-1].size(0)):\n",
    "                if self.nextYs[-1][i] == self._eos:\n",
    "                    beamLk[i] = -1e20\n",
    "        else:\n",
    "            beamLk = wordLk[0]\n",
    "        flatBeamLk = beamLk.view(-1)\n",
    "        bestScores, bestScoresId = flatBeamLk.topk(self.size, 0, True, True)\n",
    "\n",
    "        self.scores = bestScores\n",
    "\n",
    "        # bestScoresId is flattened beam x word array, so calculate which\n",
    "        # word and beam each score came from\n",
    "        prevK = bestScoresId // numWords\n",
    "        self.prevKs.append(prevK)\n",
    "        self.nextYs.append((bestScoresId - prevK * numWords))\n",
    "\n",
    "\n",
    "        for i in range(self.nextYs[-1].size(0)):\n",
    "            if self.nextYs[-1][i] == self._eos:\n",
    "                s = self.scores[i]\n",
    "                self.finished.append((s, len(self.nextYs) - 1, i))\n",
    "\n",
    "        # End condition is when top-of-beam is EOS and no global score.\n",
    "        if self.nextYs[-1][0] == self._eos:\n",
    "            self.eosTop = True\n",
    "\n",
    "    def done(self):\n",
    "        return self.eosTop and len(self.finished) >=self.size\n",
    "\n",
    "    def getFinal(self):\n",
    "        if len(self.finished) == 0:\n",
    "            self.finished.append((self.scores[0], len(self.nextYs) - 1, 0))\n",
    "        self.finished.sort(key=lambda a: -a[0])\n",
    "        if len(self.finished) != self.size:\n",
    "            unfinished=[]\n",
    "            for i in range(self.nextYs[-1].size(0)):\n",
    "                if self.nextYs[-1][i] != self._eos:\n",
    "                    s = self.scores[i]\n",
    "                    unfinished.append((s, len(self.nextYs) - 1, i)) \n",
    "            unfinished.sort(key=lambda a: -a[0])\n",
    "            self.finished+=unfinished[:self.size-len(self.finished)]\n",
    "        return self.finished[:self.size]\n",
    "\n",
    "    def getHyp(self, beam_res):\n",
    "        \"\"\"\n",
    "        Walk back to construct the full hypothesis.\n",
    "        \"\"\"\n",
    "        hyps=[]\n",
    "        for _,timestep, k in beam_res:\n",
    "            hyp = []\n",
    "            for j in range(len(self.prevKs[:timestep]) - 1, -1, -1):\n",
    "                hyp.append(self.nextYs[j+1][k])\n",
    "                k = self.prevKs[j][k]\n",
    "            hyps.append(hyp[::-1])\n",
    "        return hyps\n",
    "    \n",
    "    def buildTargetTokens(self, preds):\n",
    "        sentence=[]\n",
    "        for pred in preds:\n",
    "            tokens = []\n",
    "            for tok in pred:\n",
    "                if tok==self._eos:\n",
    "                    break\n",
    "                tokens.append(tok)\n",
    "            sentence.append(tokens)\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBYb4zUY4izs"
   },
   "source": [
    "### Model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "seWZ-IHbjKjA",
    "outputId": "1f6fe69c-13da-4ff4-9f72-d2fdf2419dd5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "MODEL_CLASSES = {'auto': (AutoConfig, AutoModel, AutoTokenizer)}\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES['auto']\n",
    "config = AutoConfig.from_pretrained(\"microsoft/graphcodebert-base\", vocab_size = 50005)#, hidden_dropout_prob=0.3)#,attention_probs_dropout_prob=0.2, hidden_dropout_prob=0.2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"uclanlp/plbart-base\")\n",
    "\n",
    "encoder = AutoModel.from_config(config)    \n",
    "decoder_layer = nn.TransformerDecoderLayer(d_model=config.hidden_size, nhead=config.num_attention_heads)#, dropout=0.5)\n",
    "decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n",
    "model=Seq2Seq(encoder=encoder,decoder=decoder,config=config,\n",
    "              beam_size=1,max_length=128,\n",
    "              sos_id=tokenizer.cls_token_id,eos_id=tokenizer.sep_token_id)\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n6JaXkhxjQEj",
    "outputId": "80046489-0c33-4298-9087-53ac0403c82c"
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "      'weight_decay': 0.0},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-4, eps=1.0)#,weight_decay=0.01)\n",
    "\n",
    "\n",
    "checkpoint = torch.load(\"./drive/MyDrive/ThesisDataset/Models/GCB96_0305001_bleu_java\")#,map_location='cpu'\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BWTp7M5mtHs8"
   },
   "outputs": [],
   "source": [
    "del checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XA8P57pUkpDm"
   },
   "source": [
    "### Input preparation for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oHpUtyCOklTx",
    "outputId": "a36fa160-5b56-4c99-a230-f27e396e3ba3"
   },
   "outputs": [],
   "source": [
    "! git clone https://github.com/AntoniosMantzaris/CodeBERT.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KFXDi06ok1uL",
    "outputId": "fa1b38cd-9a5b-4c4a-e1f7-44755262bd86"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(\"CodeBERT/GraphCodeBERT/translation\")\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BgP7hYjVk6UV"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "from io import open\n",
    "from itertools import cycle\n",
    "import torch.nn as nn\n",
    "from model import Seq2Seq\n",
    "from tqdm import tqdm, trange\n",
    "from bleu import _bleu\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from transformers import (WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup,\n",
    "                          AutoConfig, AutoModel, AutoTokenizer, PLBartTokenizer)\n",
    "MODEL_CLASSES = {'auto': (AutoConfig, AutoModel, AutoTokenizer)}\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "from parser import DFG_python,DFG_java,DFG_ruby,DFG_go,DFG_php,DFG_javascript,DFG_csharp\n",
    "from parser import (remove_comments_and_docstrings,\n",
    "                   tree_to_token_index,\n",
    "                   index_to_code_token,\n",
    "                   tree_to_variable_index)\n",
    "from tree_sitter import Language, Parser\n",
    "dfg_function={\n",
    "    'python':DFG_python,\n",
    "    'java':DFG_java,\n",
    "    'ruby':DFG_ruby,\n",
    "    'go':DFG_go,\n",
    "    'php':DFG_php,\n",
    "    'javascript':DFG_javascript,\n",
    "    'c_sharp':DFG_csharp,\n",
    "}\n",
    "\n",
    "source_lang = \"java\"\n",
    "max_source_length = 320\n",
    "max_target_length = 128\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "#load parsers\n",
    "parsers={}\n",
    "for lang in dfg_function:\n",
    "    LANGUAGE = Language('parser/my-languages.so', lang)\n",
    "    parser = Parser()\n",
    "    parser.set_language(LANGUAGE)\n",
    "    parser = [parser,dfg_function[lang]]\n",
    "    parsers[lang]= parser\n",
    "\n",
    "#remove comments, tokenize code and extract dataflow\n",
    "def extract_dataflow(code, parser,lang):\n",
    "    #remove comments\n",
    "    try:\n",
    "        code=remove_comments_and_docstrings(code,lang)\n",
    "    except:\n",
    "        pass\n",
    "    #obtain dataflow\n",
    "    if lang==\"php\":\n",
    "        code=\"<?php\"+code+\"?>\"\n",
    "    try:\n",
    "        tree = parser[0].parse(bytes(code,'utf8'))\n",
    "        root_node = tree.root_node\n",
    "        tokens_index=tree_to_token_index(root_node)\n",
    "        code=code.split('\\n')\n",
    "        code_tokens=[index_to_code_token(x,code) for x in tokens_index]\n",
    "        index_to_code={}\n",
    "        for idx,(index,code) in enumerate(zip(tokens_index,code_tokens)):\n",
    "            index_to_code[index]=(idx,code)\n",
    "        try:\n",
    "            DFG,_=parser[1](root_node,index_to_code,{})\n",
    "        except:\n",
    "            DFG=[]\n",
    "        DFG=sorted(DFG,key=lambda x:x[1])\n",
    "        indexs=set()\n",
    "        for d in DFG:\n",
    "            if len(d[-1])!=0:\n",
    "                indexs.add(d[1])\n",
    "            for x in d[-1]:\n",
    "                indexs.add(x)\n",
    "        new_DFG=[]\n",
    "        for d in DFG:\n",
    "            if d[1] in indexs:\n",
    "                new_DFG.append(d)\n",
    "        dfg=new_DFG\n",
    "    except:\n",
    "        dfg=[]\n",
    "    return code_tokens,dfg\n",
    "\n",
    "\n",
    "class Example(object):\n",
    "    \"\"\"A single training/test example.\"\"\"\n",
    "    def __init__(self,\n",
    "                 idx,\n",
    "                 source,\n",
    "                 target,\n",
    "                 ):\n",
    "        self.idx = idx\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "\n",
    "def read_examples(filename):\n",
    "    \"\"\"Read examples from filename.\"\"\"\n",
    "    examples=[]\n",
    "    with open(filename,encoding=\"utf-8\") as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            line=line.strip()\n",
    "            js=json.loads(line)\n",
    "            if 'idx' not in js:\n",
    "                js['idx']=idx\n",
    "            nl=''.join(js['docstring_tokens']).replace('\\n',' ').replace(\"'\",'')\n",
    "            nl=''.join(nl.strip().split()).replace(',',' ')[1:-1]\n",
    "            code = js[\"code\"]\n",
    "            examples.append(\n",
    "                Example(\n",
    "                        idx = idx,\n",
    "                        source=code,\n",
    "                        target = nl,\n",
    "                        )\n",
    "            )\n",
    "    return examples\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single training/test features for a example.\"\"\"\n",
    "    def __init__(self,\n",
    "                 example_id,\n",
    "                 source_ids,\n",
    "                 position_idx,\n",
    "                 dfg_to_code,\n",
    "                 dfg_to_dfg,\n",
    "                 target_ids,\n",
    "                 source_mask,\n",
    "                 target_mask,\n",
    "\n",
    "    ):\n",
    "        self.example_id = example_id\n",
    "        self.source_ids = source_ids\n",
    "        self.position_idx = position_idx\n",
    "        self.dfg_to_code = dfg_to_code\n",
    "        self.dfg_to_dfg = dfg_to_dfg\n",
    "        self.target_ids = target_ids\n",
    "        self.source_mask = source_mask\n",
    "        self.target_mask = target_mask\n",
    "\n",
    "\n",
    "parsers={}\n",
    "for lang in dfg_function:\n",
    "    LANGUAGE = Language('parser/my-languages.so', lang)\n",
    "    parser = Parser()\n",
    "    parser.set_language(LANGUAGE)\n",
    "    parser = [parser,dfg_function[lang]]\n",
    "    parsers[lang]= parser\n",
    "\n",
    "def convert_examples_to_features(examples, tokenizer,stage=None):\n",
    "    features = []\n",
    "    for example_index, example in enumerate(tqdm(examples,total=len(examples))):\n",
    "        ##extract data flow\n",
    "        code_tokens,dfg=extract_dataflow(example.source,\n",
    "                                         parsers[\"c_sharp\" if source_lang == \"cs\" else \"java\"],\n",
    "                                         \"c_sharp\" if source_lang == \"cs\" else \"java\")\n",
    "        code_tokens=[tokenizer.tokenize('@ '+x)[1:] if idx!=0 else tokenizer.tokenize(x) for idx,x in enumerate(code_tokens)]\n",
    "        ori2cur_pos={}\n",
    "        ori2cur_pos[-1]=(0,0)\n",
    "        for i in range(len(code_tokens)):\n",
    "            ori2cur_pos[i]=(ori2cur_pos[i-1][1],ori2cur_pos[i-1][1]+len(code_tokens[i]))\n",
    "        code_tokens=[y for x in code_tokens for y in x]\n",
    "\n",
    "        #truncating\n",
    "        code_tokens=code_tokens[:max_source_length-3][:512-3]\n",
    "        source_tokens =[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]\n",
    "        source_ids =  tokenizer.convert_tokens_to_ids(source_tokens)\n",
    "        position_idx = [i+tokenizer.pad_token_id + 1 for i in range(len(source_tokens))]\n",
    "        dfg=dfg[:max_source_length-len(source_tokens)]\n",
    "        source_tokens+=[x[0] for x in dfg]\n",
    "        position_idx+=[0 for x in dfg]\n",
    "        source_ids+=[tokenizer.unk_token_id for x in dfg]\n",
    "        padding_length=max_source_length-len(source_ids)\n",
    "        position_idx+=[tokenizer.pad_token_id]*padding_length\n",
    "        source_ids+=[tokenizer.pad_token_id]*padding_length\n",
    "        source_mask = [1] * (len(source_tokens))\n",
    "        source_mask+=[0]*padding_length\n",
    "\n",
    "        #reindex\n",
    "        reverse_index={}\n",
    "        for idx,x in enumerate(dfg):\n",
    "            reverse_index[x[1]]=idx\n",
    "        for idx,x in enumerate(dfg):\n",
    "            dfg[idx]=x[:-1]+([reverse_index[i] for i in x[-1] if i in reverse_index],)\n",
    "        dfg_to_dfg=[x[-1] for x in dfg]\n",
    "        dfg_to_code=[ori2cur_pos[x[1]] for x in dfg]\n",
    "        length=len([tokenizer.cls_token])\n",
    "        dfg_to_code=[(x[0]+length,x[1]+length) for x in dfg_to_code]\n",
    "\n",
    "\n",
    "        #target\n",
    "        if stage==\"test\":\n",
    "            target_tokens = tokenizer.tokenize(\"None\")\n",
    "        else:\n",
    "            target_tokens = tokenizer.tokenize(example.target)[:max_target_length-2]\n",
    "        target_tokens = [tokenizer.cls_token]+target_tokens+[tokenizer.sep_token]\n",
    "        target_ids = tokenizer.convert_tokens_to_ids(target_tokens)\n",
    "        target_mask = [1] *len(target_ids)\n",
    "        padding_length = max_target_length - len(target_ids)\n",
    "        target_ids+=[tokenizer.pad_token_id]*padding_length\n",
    "        target_mask+=[0]*padding_length\n",
    "\n",
    "        if example_index < 5:\n",
    "            if stage=='train':\n",
    "                logger.info(\"*** Example ***\")\n",
    "                \n",
    "                logger.info(\"idx: {}\".format(example.idx))\n",
    "\n",
    "                logger.info(\"source_tokens: {}\".format([x.replace('\\u0120','_') for x in source_tokens]))\n",
    "                logger.info(\"source_ids: {}\".format(' '.join(map(str, source_ids))))\n",
    "                logger.info(\"source_mask: {}\".format(' '.join(map(str, source_mask))))\n",
    "\n",
    "                logger.info(\"target_tokens: {}\".format([x.replace('\\u0120','_') for x in target_tokens]))\n",
    "                logger.info(\"target_ids: {}\".format(' '.join(map(str, target_ids))))\n",
    "                logger.info(\"target_mask: {}\".format(' '.join(map(str, target_mask))))\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                 example_index,\n",
    "                 source_ids,\n",
    "                 position_idx,\n",
    "                 dfg_to_code,\n",
    "                 dfg_to_dfg,\n",
    "                 target_ids,\n",
    "                 source_mask,\n",
    "                 target_mask,\n",
    "            )\n",
    "        )\n",
    "    return features\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        #calculate graph-guided masked function\n",
    "        attn_mask=np.zeros((max_source_length,max_source_length),dtype=np.bool) #was self.max_length\n",
    "        #calculate begin index of node and max length of input\n",
    "        node_index=sum([i>1 for i in self.examples[item].position_idx])\n",
    "        max_length=sum([i!=1 for i in self.examples[item].position_idx])\n",
    "        #sequence can attend to sequence\n",
    "        attn_mask[:node_index,:node_index]=True\n",
    "        #special tokens attend to all tokens\n",
    "        for idx,i in enumerate(self.examples[item].source_ids):\n",
    "            if i in [0,2]:\n",
    "                attn_mask[idx,:max_length]=True\n",
    "        #nodes attend to code tokens that are identified from\n",
    "        for idx,(a,b) in enumerate(self.examples[item].dfg_to_code):\n",
    "            if a<node_index and b<node_index:\n",
    "                attn_mask[idx+node_index,a:b]=True\n",
    "                attn_mask[a:b,idx+node_index]=True\n",
    "        #nodes attend to adjacent nodes\n",
    "        for idx,nodes in enumerate(self.examples[item].dfg_to_dfg):\n",
    "            for a in nodes:\n",
    "                if a+node_index<len(self.examples[item].position_idx):\n",
    "                    attn_mask[idx+node_index,a+node_index]=True\n",
    "\n",
    "        return (torch.tensor(self.examples[item].source_ids),\n",
    "                torch.tensor(self.examples[item].source_mask),\n",
    "                torch.tensor(self.examples[item].position_idx),\n",
    "                torch.tensor(attn_mask),\n",
    "                torch.tensor(self.examples[item].target_ids),\n",
    "                torch.tensor(self.examples[item].target_mask),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGqqD3GblROI"
   },
   "outputs": [],
   "source": [
    "os.chdir(\"../../..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-C1V8M5NV_Z"
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fPUmFRoklHiJ",
    "outputId": "5f1a5584-66ef-46e8-fa0c-e387b341358f"
   },
   "outputs": [],
   "source": [
    "test_filename=\"dataset/java/testing100.jsonl\" \n",
    "\n",
    "logits = []\n",
    "files=[]\n",
    "files.append(test_filename)\n",
    "for idx,file in enumerate(files):\n",
    "  torch.cuda.empty_cache() \n",
    "  logger.info(\"Test file: {}\".format(file))\n",
    "  eval_examples = read_examples(file)\n",
    "  eval_features = convert_examples_to_features(eval_examples, tokenizer, stage='test')\n",
    "  eval_data = TextDataset(eval_features)\n",
    "\n",
    "  eval_dataloader = DataLoader(eval_data,batch_size=1)\n",
    "  p=[]\n",
    "  for batch in tqdm(eval_dataloader,total=len(eval_dataloader)):\n",
    "      batch = tuple(t.to(\"cuda\") for t in batch)\n",
    "      source_ids,source_mask,position_idx,att_mask,target_ids,target_mask = batch\n",
    "      with torch.no_grad():\n",
    "          logs, preds = model(source_ids,source_mask,position_idx,att_mask) #0:logits, 1:preds\n",
    "          logits.append(logs.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RTCEx3Lsz6N2"
   },
   "outputs": [],
   "source": [
    "soft1 = []\n",
    "pr = []\n",
    "for ar in range(len(logits)):\n",
    "  log_tens = torch.Tensor(logits[ar])\n",
    "  softmaxed = torch.exp(log_tens)\n",
    "  soft1.append(softmaxed)\n",
    "  predictiones = torch.argmax(softmaxed, 1)\n",
    "  pr.append(predictiones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zXMbftViuVfn",
    "outputId": "a0d1b935-3843-45c5-eb42-5dd41309ec0c"
   },
   "outputs": [],
   "source": [
    "temp = []\n",
    "temp.append([0]*50005)\n",
    "for i in range(len(soft1)):\n",
    "  for length in range(128-len(soft1[i])):\n",
    "    soft1[i] = torch.cat((soft1[i],torch.tensor(temp)),0)\n",
    "soft1 #the softmax that will be used for the ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WyGER5296u8m",
    "outputId": "8a91d3ef-d159-4995-91c0-52282bf38646"
   },
   "outputs": [],
   "source": [
    "gcb_preds = tokenizer.batch_decode(pr, skip_special_tokens=True)\n",
    "gcb_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0PArMgNRFhAt"
   },
   "outputs": [],
   "source": [
    "reference = []\n",
    "for i in range(100):\n",
    "  reference.append(test_java.docstring2.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ormU7wtlFel8",
    "outputId": "7992cb17-84b8-4d1b-d23d-5cb39e4ed909"
   },
   "outputs": [],
   "source": [
    "reference[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OVfBrGVjezr"
   },
   "source": [
    "## PLBart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pGtuqHPVjhbQ"
   },
   "outputs": [],
   "source": [
    "from transformers import PLBartForConditionalGeneration, PLBartTokenizer, PLBartConfig\n",
    "config =PLBartConfig.from_pretrained(\"uclanlp/plbart-base\")#,dropout=0.2, attention_dropout=0.2) \n",
    "\n",
    "tokenizer = PLBartTokenizer.from_pretrained(\"uclanlp/plbart-base\")\n",
    "modelo = PLBartForConditionalGeneration.from_pretrained(\"uclanlp/plbart-base\")\n",
    "#modelo = PLBartForConditionalGeneration(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RamdnIrWkGNr",
    "outputId": "a26ba322-a54a-43d7-cfe1-02ad262b8674"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "my_model = PLBartForConditionalGeneration(config)\n",
    "checkpoint2 = torch.load('./drive/MyDrive/ThesisDataset/Models/PLBartMed2_last_java')\n",
    "my_model.load_state_dict(checkpoint2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Acx4vpzhC3wm"
   },
   "outputs": [],
   "source": [
    "del checkpoint2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4gP89Dew70MS"
   },
   "outputs": [],
   "source": [
    "def preprocessPL(data):\n",
    "    return tokenizer(data['code2'], max_length = 320, padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2_LsC82799b"
   },
   "outputs": [],
   "source": [
    "from transformers import generation_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ru9FKJKI8fT7"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "sfm = nn.Softmax(dim=-1)\n",
    "\n",
    "soft2 = []\n",
    "final = []\n",
    "reference = []\n",
    "for i in range(100):\n",
    "  input_tensor = preprocessPL(test_java.iloc[i])\n",
    "  reference.append(test_java.docstring2.iloc[i])\n",
    "  generation = my_model.generate(**input_tensor,return_dict_in_generate=True, output_scores=True)\n",
    "  logi = generation.scores\n",
    "  \n",
    "  softs = [] #for logits (softmaxed)\n",
    "  softi = []\n",
    "  for tens in logi:\n",
    "    s = sfm(tens)\n",
    "    softs.append(s[0])\n",
    "    softi.append(torch.argmax(s,-1).item())\n",
    "  soft2.append(softs)\n",
    "  final.append(softi)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JqdayjJBErMr"
   },
   "outputs": [],
   "source": [
    "for i in range(len(soft2)):\n",
    "  soft2[i]=torch.stack(soft2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cgqyqk7nAhFC"
   },
   "outputs": [],
   "source": [
    "temp = []\n",
    "temp.append([0]*50005)\n",
    "for i in range(len(soft2)):\n",
    "  for length in range(128-len(soft2[i])):\n",
    "    soft2[i] = torch.cat((soft2[i],torch.tensor(temp)),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9NL4wqwO84GZ"
   },
   "outputs": [],
   "source": [
    "plbart_preds = []\n",
    "for l in final:\n",
    "  temp = []\n",
    "  temp.append(l)\n",
    "  t = torch.IntTensor(temp) #this way i can get the sentence and not the token sequence\n",
    "  print(tokenizer.batch_decode(t, skip_special_tokens=True)[0])\n",
    "  plbart_preds.append(tokenizer.batch_decode(t, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yjTloT8nCJDX",
    "outputId": "8df08f0a-5804-451c-cd44-c206cd711c3c"
   },
   "outputs": [],
   "source": [
    "plbart_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sirNrDl_CMgO",
    "outputId": "13941ccc-1039-44f2-f620-646e6e7a3c91"
   },
   "outputs": [],
   "source": [
    "reference[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0T-B8f89v_d"
   },
   "source": [
    "## BLEU-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dHpB7Bit9zQG",
    "outputId": "0aa71434-0b2d-4948-bf0d-12119767ce6c"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "'''\n",
    "This script was adapted from the original version by hieuhoang1972 which is part of MOSES. \n",
    "'''\n",
    "\n",
    "# $Id: bleu.py 1307 2007-03-14 22:22:36Z hieuhoang1972 $\n",
    "\n",
    "'''Provides:\n",
    "\n",
    "cook_refs(refs, n=4): Transform a list of reference sentences as strings into a form usable by cook_test().\n",
    "cook_test(test, refs, n=4): Transform a test sentence as a string (together with the cooked reference sentences) into a form usable by score_cooked().\n",
    "score_cooked(alltest, n=4): Score a list of cooked test sentences.\n",
    "\n",
    "score_set(s, testid, refids, n=4): Interface with dataset.py; calculate BLEU score of testid against refids.\n",
    "\n",
    "The reason for breaking the BLEU computation into three phases cook_refs(), cook_test(), and score_cooked() is to allow the caller to calculate BLEU scores for multiple test sets as efficiently as possible.\n",
    "'''\n",
    "\n",
    "import sys, math, re, xml.sax.saxutils\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Added to bypass NIST-style pre-processing of hyp and ref files -- wade\n",
    "nonorm = 0\n",
    "\n",
    "preserve_case = False\n",
    "eff_ref_len = \"shortest\"\n",
    "\n",
    "normalize1 = [\n",
    "    ('<skipped>', ''),         # strip \"skipped\" tags\n",
    "    (r'-\\n', ''),              # strip end-of-line hyphenation and join lines\n",
    "    (r'\\n', ' '),              # join lines\n",
    "#    (r'(\\d)\\s+(?=\\d)', r'\\1'), # join digits\n",
    "]\n",
    "normalize1 = [(re.compile(pattern), replace) for (pattern, replace) in normalize1]\n",
    "\n",
    "normalize2 = [\n",
    "    (r'([\\{-\\~\\[-\\` -\\&\\(-\\+\\:-\\@\\/])',r' \\1 '), # tokenize punctuation. apostrophe is missing\n",
    "    (r'([^0-9])([\\.,])',r'\\1 \\2 '),              # tokenize period and comma unless preceded by a digit\n",
    "    (r'([\\.,])([^0-9])',r' \\1 \\2'),              # tokenize period and comma unless followed by a digit\n",
    "    (r'([0-9])(-)',r'\\1 \\2 ')                    # tokenize dash when preceded by a digit\n",
    "]\n",
    "normalize2 = [(re.compile(pattern), replace) for (pattern, replace) in normalize2]\n",
    "\n",
    "def normalize(s):\n",
    "    '''Normalize and tokenize text. This is lifted from NIST mteval-v11a.pl.'''\n",
    "    # Added to bypass NIST-style pre-processing of hyp and ref files -- wade\n",
    "    if (nonorm):\n",
    "        return s.split()\n",
    "    if type(s) is not str:\n",
    "        s = \" \".join(s)\n",
    "    # language-independent part:\n",
    "    for (pattern, replace) in normalize1:\n",
    "        s = re.sub(pattern, replace, s)\n",
    "    s = xml.sax.saxutils.unescape(s, {'&quot;':'\"'})\n",
    "    # language-dependent part (assuming Western languages):\n",
    "    s = \" %s \" % s\n",
    "    if not preserve_case:\n",
    "        s = s.lower()         # this might not be identical to the original\n",
    "    for (pattern, replace) in normalize2:\n",
    "        s = re.sub(pattern, replace, s)\n",
    "    return s.split()\n",
    "\n",
    "def count_ngrams(words, n=4):\n",
    "    counts = {}\n",
    "    for k in range(1,n+1):\n",
    "        for i in range(len(words)-k+1):\n",
    "            ngram = tuple(words[i:i+k])\n",
    "            counts[ngram] = counts.get(ngram, 0)+1\n",
    "    return counts\n",
    "\n",
    "def cook_refs(refs, n=4):\n",
    "    '''Takes a list of reference sentences for a single segment\n",
    "    and returns an object that encapsulates everything that BLEU\n",
    "    needs to know about them.'''\n",
    "    \n",
    "    refs = [normalize(ref) for ref in refs]\n",
    "    maxcounts = {}\n",
    "    for ref in refs:\n",
    "        counts = count_ngrams(ref, n)\n",
    "        for (ngram,count) in counts.items():\n",
    "            maxcounts[ngram] = max(maxcounts.get(ngram,0), count)\n",
    "    return ([len(ref) for ref in refs], maxcounts)\n",
    "\n",
    "def cook_test(test, item, n=4):\n",
    "    '''Takes a test sentence and returns an object that\n",
    "    encapsulates everything that BLEU needs to know about it.'''\n",
    "    (reflens, refmaxcounts)=item\n",
    "    test = normalize(test)\n",
    "    result = {}\n",
    "    result[\"testlen\"] = len(test)\n",
    "\n",
    "    # Calculate effective reference sentence length.\n",
    "    \n",
    "    if eff_ref_len == \"shortest\":\n",
    "        result[\"reflen\"] = min(reflens)\n",
    "    elif eff_ref_len == \"average\":\n",
    "        result[\"reflen\"] = float(sum(reflens))/len(reflens)\n",
    "    elif eff_ref_len == \"closest\":\n",
    "        min_diff = None\n",
    "        for reflen in reflens:\n",
    "            if min_diff is None or abs(reflen-len(test)) < min_diff:\n",
    "                min_diff = abs(reflen-len(test))\n",
    "                result['reflen'] = reflen\n",
    "\n",
    "    result[\"guess\"] = [max(len(test)-k+1,0) for k in range(1,n+1)]\n",
    "\n",
    "    result['correct'] = [0]*n\n",
    "    counts = count_ngrams(test, n)\n",
    "    for (ngram, count) in counts.items():\n",
    "        result[\"correct\"][len(ngram)-1] += min(refmaxcounts.get(ngram,0), count)\n",
    "\n",
    "    return result\n",
    "\n",
    "def score_cooked(allcomps, n=4, ground=0, smooth=1):\n",
    "    totalcomps = {'testlen':0, 'reflen':0, 'guess':[0]*n, 'correct':[0]*n}\n",
    "    for comps in allcomps:\n",
    "        for key in ['testlen','reflen']:\n",
    "            totalcomps[key] += comps[key]\n",
    "        for key in ['guess','correct']:\n",
    "            for k in range(n):\n",
    "                totalcomps[key][k] += comps[key][k]\n",
    "    logbleu = 0.0\n",
    "    all_bleus = []\n",
    "    for k in range(n):\n",
    "      correct = totalcomps['correct'][k]\n",
    "      guess = totalcomps['guess'][k]\n",
    "      addsmooth = 0\n",
    "      if smooth == 1 and k > 0:\n",
    "        addsmooth = 1\n",
    "      logbleu += math.log(correct + addsmooth + sys.float_info.min)-math.log(guess + addsmooth+ sys.float_info.min)\n",
    "      if guess == 0:\n",
    "        all_bleus.append(-10000000)\n",
    "      else:\n",
    "        all_bleus.append(math.log(correct + sys.float_info.min)-math.log( guess ))\n",
    "\n",
    "    logbleu /= float(n)\n",
    "    all_bleus.insert(0, logbleu)\n",
    "\n",
    "    brevPenalty = min(0,1-float(totalcomps['reflen'] + 1)/(totalcomps['testlen'] + 1))\n",
    "    for i in range(len(all_bleus)):\n",
    "      if i ==0:\n",
    "        all_bleus[i] += brevPenalty\n",
    "      all_bleus[i] = math.exp(all_bleus[i])\n",
    "    return all_bleus\n",
    "\n",
    "def bleu(refs,  candidate, ground=0, smooth=1):\n",
    "    refs = cook_refs(refs)\n",
    "    test = cook_test(candidate, refs)\n",
    "    return score_cooked([test], ground=ground, smooth=smooth)\n",
    "\n",
    "def splitPuncts(line):\n",
    "  return ' '.join(re.findall(r\"[\\w]+|[^\\s\\w]\", line))\n",
    "\n",
    "def computeMaps(predictions, goldfile):\n",
    "  predictionMap = {}\n",
    "  goldMap = {}\n",
    "  gf = goldfile #open(goldfile, 'r')\n",
    "\n",
    "  id = 0\n",
    "  for row in predictions:\n",
    "    cols = row.strip().split('\\t') \n",
    "    (rid, pred) = (id, cols[0])\n",
    "    id+=1\n",
    "    \"\"\"\n",
    "    if len(cols) == 1:\n",
    "      (rid, pred) = (cols[0], '') \n",
    "    else:\n",
    "      (rid, pred) = (cols[0], cols[1])\"\"\"\n",
    "    predictionMap[rid] = [splitPuncts(pred.strip().lower())]\n",
    "    \n",
    "  id = 0\n",
    "  for row in gf:\n",
    "    cols = row.strip().split('\\t')\n",
    "    (rid, pred) = (id, cols[0])\n",
    "    id+=1\n",
    "    \"\"\"if len(cols) == 1:\n",
    "      (rid, pred) = (cols[0], '') \n",
    "    else:\n",
    "      (rid, pred) = (cols[0], cols[1])\"\"\" \n",
    "    #goldMap[rid] = [splitPuncts(pred.strip().lower())]\n",
    "    #(rid, pred) = row.split('\\t') \n",
    "    if rid in predictionMap: # Only insert if the id exists for the method\n",
    "      if rid not in goldMap:\n",
    "        goldMap[rid] = []\n",
    "      goldMap[rid].append(splitPuncts(pred.strip().lower()))\n",
    "    #print(goldMap)\n",
    "  sys.stderr.write('Total: ' + str(len(goldMap)) + '\\n')\n",
    "  return (goldMap, predictionMap)\n",
    "\n",
    "\n",
    "#m1 is the reference map\n",
    "#m2 is the prediction map\n",
    "def bleuFromMaps(m1, m2):\n",
    "  score = [0] * 5\n",
    "  num = 0.0\n",
    "\n",
    "  for key in m1:\n",
    "    if key in m2:\n",
    "      bl = bleu(m1[key], m2[key][0])\n",
    "      score = [ score[i] + bl[i] for i in range(0, len(bl))]\n",
    "      num += 1\n",
    "  return [s * 100.0 / num for s in score]\n",
    "\n",
    "\"\"\"if __name__ == '__main__':\n",
    "  reference_file = sys.argv[1]\n",
    "  predictions = []\n",
    "  for row in sys.stdin:\n",
    "    predictions.append(row)\"\"\"\n",
    "(goldMap, predictionMap) = computeMaps(plbart_preds, reference) \n",
    "print(bleuFromMaps(goldMap, predictionMap)[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLwrR95Ilptt"
   },
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZBl0_agl9m04"
   },
   "outputs": [],
   "source": [
    "\"\"\"del gcb_preds\n",
    "del plbart_preds\n",
    "del temp\n",
    "del input_tensor\n",
    "del generation\n",
    "del final\n",
    "del p\n",
    "del pr\n",
    "del model\n",
    "del my_model\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RtC425vH0XEz"
   },
   "source": [
    "### Sentence union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O4dozDo-0pdE"
   },
   "outputs": [],
   "source": [
    "union_pred = []\n",
    "for i in range(100):\n",
    "  union_pred.append(plbart_preds[i]+\" \" +gcb_preds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7LUALl8c1ELz",
    "outputId": "0cacf427-3627-4b8b-ade5-a8b2bd709d28"
   },
   "outputs": [],
   "source": [
    "union_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9SY-ui_MhqY"
   },
   "source": [
    "### Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7KPLEDV8P8Au"
   },
   "outputs": [],
   "source": [
    "def mean_ens():\n",
    "  avg =[]\n",
    "  for idx in range(100):\n",
    "    sum = torch.add(soft1[idx], soft2[idx], alpha=1)  #*alpha the softmaxed2/ just for this testing\n",
    "    avg.append(torch.div(sum,2))\n",
    "\n",
    "  preds_avg = []\n",
    "  for i in range(len(avg)):\n",
    "    preds_avg.append(torch.argmax(torch.tensor(avg[i]), -1).tolist())\n",
    "\n",
    "  mean_preds = []\n",
    "  for l in preds_avg:\n",
    "    temp = []\n",
    "    temp.append(l)\n",
    "    t = torch.IntTensor(temp) #this way i can get the sentence and not the token sequence\n",
    "    mean_preds.append(tokenizer.batch_decode(t, skip_special_tokens=True)[0])\n",
    "\n",
    "  return mean_preds   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zIZyfFyIQfxe",
    "outputId": "7bee524e-1198-43b5-d934-470a9df7366c"
   },
   "outputs": [],
   "source": [
    "preds1 = mean_ens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QjJGtMH4GNjq",
    "outputId": "8da4ee46-4327-4634-c80e-815e11feab5b"
   },
   "outputs": [],
   "source": [
    "preds1[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-vs8TovMnDt"
   },
   "source": [
    "### Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2MGp_iipRI6t"
   },
   "outputs": [],
   "source": [
    "def max_ens():\n",
    "  maximum =[]\n",
    "  for idx in range(100):\n",
    "    maximum.append(torch.max(soft1[idx], soft2[idx]))\n",
    "  preds_max = []\n",
    "  \n",
    "  for i in range(len(maximum)):\n",
    "    preds_max.append(torch.argmax(torch.tensor(maximum[i]), -1).tolist())\n",
    "   \n",
    "  max_preds = []\n",
    "  for l in preds_max:\n",
    "    temp = []\n",
    "    temp.append(l)\n",
    "    t = torch.IntTensor(temp) #this way i can get the sentence and not the token sequence\n",
    "    max_preds.append(tokenizer.batch_decode(t, skip_special_tokens=True)[0])\n",
    "\n",
    "  return max_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IF-YGuYSRpGV",
    "outputId": "c1865d04-daac-4282-a083-0c03f2b62b2e"
   },
   "outputs": [],
   "source": [
    "preds2 = max_ens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8cmEgAFiRtZd"
   },
   "outputs": [],
   "source": [
    "preds2[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODtz4A-YkssU"
   },
   "source": [
    "### Weighted mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pPDQV4Z_R_M2"
   },
   "outputs": [],
   "source": [
    "def weighted_mean(weightGCB, weightPLBart):\n",
    "  avg_weighted =[]\n",
    "  for idx in range(100):\n",
    "    sum_w = torch.add(weightGCB*soft1[idx], weightPLBart*soft2[idx], alpha=1)  #*alpha the softmaxed2/ just for this testing\n",
    "    avg_weighted.append(torch.div(sum_w,weightGCB+weightPLBart))\n",
    "\n",
    "  preds_weighted_avg = []\n",
    "  for i in range(len(avg_weighted)):\n",
    "    preds_weighted_avg.append(torch.argmax(torch.tensor(avg_weighted[i]), -1).tolist())\n",
    "\n",
    "  weighted_mean_preds = []\n",
    "  for l in preds_weighted_avg:\n",
    "    temp = []\n",
    "    temp.append(l)\n",
    "    t = torch.IntTensor(temp) #this way i can get the sentence and not the token sequence\n",
    "    weighted_mean_preds.append(tokenizer.batch_decode(t, skip_special_tokens=True)[0])\n",
    "\n",
    "  return weighted_mean_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KIP2rbzUSuk4",
    "outputId": "ef8dbd08-0a72-4a89-f79c-fac42975421f"
   },
   "outputs": [],
   "source": [
    "preds3 = weighted_mean(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I0-sSLNdS33v",
    "outputId": "ed0122b1-fcc5-4293-d05f-34b41c63c51c"
   },
   "outputs": [],
   "source": [
    "preds3[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uScDGRHw_cSu",
    "outputId": "663d894f-2126-44fd-9a00-da2b710efb84"
   },
   "outputs": [],
   "source": [
    "reference[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gcxsw5fGre3E"
   },
   "source": [
    "### Weighted max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2EiyXnOtR8DQ"
   },
   "outputs": [],
   "source": [
    "def weighted_max(weightGCB,weightPLBart):\n",
    "  w_maximum =[]\n",
    "  for idx in range(100):\n",
    "    w_maximum.append(torch.max(weightGCB*soft1[idx], weightPLBart*soft2[idx]))  \n",
    "\n",
    "  preds_max_w = []\n",
    "  for i in range(len(w_maximum)):\n",
    "    preds_max_w.append(torch.argmax(torch.tensor(w_maximum[i]), -1).tolist())\n",
    "\n",
    "  weighted_max_preds = []\n",
    "  for l in preds_max_w:\n",
    "    temp = []\n",
    "    temp.append(l)\n",
    "    t = torch.IntTensor(temp) #this way i can get the sentence and not the token sequence\n",
    "    weighted_max_preds.append(tokenizer.batch_decode(t, skip_special_tokens=True)[0])\n",
    "\n",
    "  return weighted_max_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_cJtmGZSTrNY",
    "outputId": "a8d27e9c-0cbc-48c7-d852-05162950cb0b"
   },
   "outputs": [],
   "source": [
    "preds4 = weighted_max(8,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hDQ0voTlTxF1",
    "outputId": "5471dab9-a50f-453c-83fb-87ed412a7896"
   },
   "outputs": [],
   "source": [
    "preds4[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i15UPghhzCB9",
    "outputId": "c419dd98-6e50-4e0b-8b4f-7c194d1a126f"
   },
   "outputs": [],
   "source": [
    "reference[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrdJ1BUa7qRZ"
   },
   "source": [
    "### DoubleEnsemble (weighted mean(weighted_max,PLBart))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-_CKfZ_YiEV"
   },
   "outputs": [],
   "source": [
    "def doubleEns(weightMaxGCB, weightMaxPLBart, weightEns, weightPLBart):\n",
    "  w_maximum =[]\n",
    "  for idx in range(100):\n",
    "    w_maximum.append(torch.max(weightMaxGCB*soft1[idx], weightMaxPLBart*soft2[idx]))\n",
    "\n",
    "  avg_weighted =[]\n",
    "  for idx in range(100):\n",
    "    sum_w = torch.add(weightEns*w_maximum[idx], weightPLBart*soft2[idx], alpha=1)  #*alpha the softmaxed2/ just for this testing\n",
    "    avg_weighted.append(torch.div(sum_w,weightEns+weightPLBart))\n",
    "\n",
    "  preds_weighted_avg2 = []\n",
    "  for i in range(len(avg_weighted)):\n",
    "    preds_weighted_avg2.append(torch.argmax(torch.tensor(avg_weighted[i]), -1).tolist())\n",
    "\n",
    "  weighted_mean_preds2 = []\n",
    "  for l in preds_weighted_avg2:\n",
    "    temp = []\n",
    "    temp.append(l)\n",
    "    t = torch.IntTensor(temp) #this way i can get the sentence and not the token sequence\n",
    "    weighted_mean_preds2.append(tokenizer.batch_decode(t, skip_special_tokens=True)[0])\n",
    "\n",
    "  return weighted_mean_preds2     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5o8NncyBapX_",
    "outputId": "49cfd2fe-0373-49d1-a50f-d4678732109f"
   },
   "outputs": [],
   "source": [
    "preds5 = doubleEns(8,1,1,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lgN-4-H4avI7",
    "outputId": "3a2cc22a-ae69-4604-a7f6-39b8cd0e61b4"
   },
   "outputs": [],
   "source": [
    "preds5[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iyl0yY5B_vSS",
    "outputId": "78ae19e6-20e7-4561-bdb1-1741f0d58a14"
   },
   "outputs": [],
   "source": [
    "reference[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcMdT_FnW7XE"
   },
   "source": [
    "## BLEU-4 (ensemble/same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8FCSSYjzW-6r",
    "outputId": "5873b9e3-d88e-44a7-fd02-37499bb18828"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "'''\n",
    "This script was adapted from the original version by hieuhoang1972 which is part of MOSES. \n",
    "'''\n",
    "\n",
    "# $Id: bleu.py 1307 2007-03-14 22:22:36Z hieuhoang1972 $\n",
    "\n",
    "'''Provides:\n",
    "\n",
    "cook_refs(refs, n=4): Transform a list of reference sentences as strings into a form usable by cook_test().\n",
    "cook_test(test, refs, n=4): Transform a test sentence as a string (together with the cooked reference sentences) into a form usable by score_cooked().\n",
    "score_cooked(alltest, n=4): Score a list of cooked test sentences.\n",
    "\n",
    "score_set(s, testid, refids, n=4): Interface with dataset.py; calculate BLEU score of testid against refids.\n",
    "\n",
    "The reason for breaking the BLEU computation into three phases cook_refs(), cook_test(), and score_cooked() is to allow the caller to calculate BLEU scores for multiple test sets as efficiently as possible.\n",
    "'''\n",
    "\n",
    "import sys, math, re, xml.sax.saxutils\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Added to bypass NIST-style pre-processing of hyp and ref files -- wade\n",
    "nonorm = 0\n",
    "\n",
    "preserve_case = False\n",
    "eff_ref_len = \"shortest\"\n",
    "\n",
    "normalize1 = [\n",
    "    ('<skipped>', ''),         # strip \"skipped\" tags\n",
    "    (r'-\\n', ''),              # strip end-of-line hyphenation and join lines\n",
    "    (r'\\n', ' '),              # join lines\n",
    "#    (r'(\\d)\\s+(?=\\d)', r'\\1'), # join digits\n",
    "]\n",
    "normalize1 = [(re.compile(pattern), replace) for (pattern, replace) in normalize1]\n",
    "\n",
    "normalize2 = [\n",
    "    (r'([\\{-\\~\\[-\\` -\\&\\(-\\+\\:-\\@\\/])',r' \\1 '), # tokenize punctuation. apostrophe is missing\n",
    "    (r'([^0-9])([\\.,])',r'\\1 \\2 '),              # tokenize period and comma unless preceded by a digit\n",
    "    (r'([\\.,])([^0-9])',r' \\1 \\2'),              # tokenize period and comma unless followed by a digit\n",
    "    (r'([0-9])(-)',r'\\1 \\2 ')                    # tokenize dash when preceded by a digit\n",
    "]\n",
    "normalize2 = [(re.compile(pattern), replace) for (pattern, replace) in normalize2]\n",
    "\n",
    "def normalize(s):\n",
    "    '''Normalize and tokenize text. This is lifted from NIST mteval-v11a.pl.'''\n",
    "    # Added to bypass NIST-style pre-processing of hyp and ref files -- wade\n",
    "    if (nonorm):\n",
    "        return s.split()\n",
    "    if type(s) is not str:\n",
    "        s = \" \".join(s)\n",
    "    # language-independent part:\n",
    "    for (pattern, replace) in normalize1:\n",
    "        s = re.sub(pattern, replace, s)\n",
    "    s = xml.sax.saxutils.unescape(s, {'&quot;':'\"'})\n",
    "    # language-dependent part (assuming Western languages):\n",
    "    s = \" %s \" % s\n",
    "    if not preserve_case:\n",
    "        s = s.lower()         # this might not be identical to the original\n",
    "    for (pattern, replace) in normalize2:\n",
    "        s = re.sub(pattern, replace, s)\n",
    "    return s.split()\n",
    "\n",
    "def count_ngrams(words, n=4):\n",
    "    counts = {}\n",
    "    for k in range(1,n+1):\n",
    "        for i in range(len(words)-k+1):\n",
    "            ngram = tuple(words[i:i+k])\n",
    "            counts[ngram] = counts.get(ngram, 0)+1\n",
    "    return counts\n",
    "\n",
    "def cook_refs(refs, n=4):\n",
    "    '''Takes a list of reference sentences for a single segment\n",
    "    and returns an object that encapsulates everything that BLEU\n",
    "    needs to know about them.'''\n",
    "    \n",
    "    refs = [normalize(ref) for ref in refs]\n",
    "    maxcounts = {}\n",
    "    for ref in refs:\n",
    "        counts = count_ngrams(ref, n)\n",
    "        for (ngram,count) in counts.items():\n",
    "            maxcounts[ngram] = max(maxcounts.get(ngram,0), count)\n",
    "    return ([len(ref) for ref in refs], maxcounts)\n",
    "\n",
    "def cook_test(test, item, n=4):\n",
    "    '''Takes a test sentence and returns an object that\n",
    "    encapsulates everything that BLEU needs to know about it.'''\n",
    "    (reflens, refmaxcounts)=item\n",
    "    test = normalize(test)\n",
    "    result = {}\n",
    "    result[\"testlen\"] = len(test)\n",
    "\n",
    "    # Calculate effective reference sentence length.\n",
    "    \n",
    "    if eff_ref_len == \"shortest\":\n",
    "        result[\"reflen\"] = min(reflens)\n",
    "    elif eff_ref_len == \"average\":\n",
    "        result[\"reflen\"] = float(sum(reflens))/len(reflens)\n",
    "    elif eff_ref_len == \"closest\":\n",
    "        min_diff = None\n",
    "        for reflen in reflens:\n",
    "            if min_diff is None or abs(reflen-len(test)) < min_diff:\n",
    "                min_diff = abs(reflen-len(test))\n",
    "                result['reflen'] = reflen\n",
    "\n",
    "    result[\"guess\"] = [max(len(test)-k+1,0) for k in range(1,n+1)]\n",
    "\n",
    "    result['correct'] = [0]*n\n",
    "    counts = count_ngrams(test, n)\n",
    "    for (ngram, count) in counts.items():\n",
    "        result[\"correct\"][len(ngram)-1] += min(refmaxcounts.get(ngram,0), count)\n",
    "\n",
    "    return result\n",
    "\n",
    "def score_cooked(allcomps, n=4, ground=0, smooth=1):\n",
    "    totalcomps = {'testlen':0, 'reflen':0, 'guess':[0]*n, 'correct':[0]*n}\n",
    "    for comps in allcomps:\n",
    "        for key in ['testlen','reflen']:\n",
    "            totalcomps[key] += comps[key]\n",
    "        for key in ['guess','correct']:\n",
    "            for k in range(n):\n",
    "                totalcomps[key][k] += comps[key][k]\n",
    "    logbleu = 0.0\n",
    "    all_bleus = []\n",
    "    for k in range(n):\n",
    "      correct = totalcomps['correct'][k]\n",
    "      guess = totalcomps['guess'][k]\n",
    "      addsmooth = 0\n",
    "      if smooth == 1 and k > 0:\n",
    "        addsmooth = 1\n",
    "      logbleu += math.log(correct + addsmooth + sys.float_info.min)-math.log(guess + addsmooth+ sys.float_info.min)\n",
    "      if guess == 0:\n",
    "        all_bleus.append(-10000000)\n",
    "      else:\n",
    "        all_bleus.append(math.log(correct + sys.float_info.min)-math.log( guess ))\n",
    "\n",
    "    logbleu /= float(n)\n",
    "    all_bleus.insert(0, logbleu)\n",
    "\n",
    "    brevPenalty = min(0,1-float(totalcomps['reflen'] + 1)/(totalcomps['testlen'] + 1))\n",
    "    for i in range(len(all_bleus)):\n",
    "      if i ==0:\n",
    "        all_bleus[i] += brevPenalty\n",
    "      all_bleus[i] = math.exp(all_bleus[i])\n",
    "    return all_bleus\n",
    "\n",
    "def bleu(refs,  candidate, ground=0, smooth=1):\n",
    "    refs = cook_refs(refs)\n",
    "    test = cook_test(candidate, refs)\n",
    "    return score_cooked([test], ground=ground, smooth=smooth)\n",
    "\n",
    "def splitPuncts(line):\n",
    "  return ' '.join(re.findall(r\"[\\w]+|[^\\s\\w]\", line))\n",
    "\n",
    "def computeMaps(predictions, goldfile):\n",
    "  predictionMap = {}\n",
    "  goldMap = {}\n",
    "  gf = goldfile #open(goldfile, 'r')\n",
    "\n",
    "  id = 0\n",
    "  for row in predictions:\n",
    "    cols = row.strip().split('\\t') \n",
    "    (rid, pred) = (id, cols[0])\n",
    "    id+=1\n",
    "    \"\"\"\n",
    "    if len(cols) == 1:\n",
    "      (rid, pred) = (cols[0], '') \n",
    "    else:\n",
    "      (rid, pred) = (cols[0], cols[1])\"\"\"\n",
    "    predictionMap[rid] = [splitPuncts(pred.strip().lower())]\n",
    "    \n",
    "  id = 0\n",
    "  for row in gf:\n",
    "    cols = row.strip().split('\\t')\n",
    "    (rid, pred) = (id, cols[0])\n",
    "    id+=1\n",
    "    \"\"\"if len(cols) == 1:\n",
    "      (rid, pred) = (cols[0], '') \n",
    "    else:\n",
    "      (rid, pred) = (cols[0], cols[1])\"\"\" \n",
    "    #goldMap[rid] = [splitPuncts(pred.strip().lower())]\n",
    "    #(rid, pred) = row.split('\\t') \n",
    "    if rid in predictionMap: # Only insert if the id exists for the method\n",
    "      if rid not in goldMap:\n",
    "        goldMap[rid] = []\n",
    "      goldMap[rid].append(splitPuncts(pred.strip().lower()))\n",
    "    #print(goldMap)\n",
    "  sys.stderr.write('Total: ' + str(len(goldMap)) + '\\n')\n",
    "  return (goldMap, predictionMap)\n",
    "\n",
    "\n",
    "#m1 is the reference map\n",
    "#m2 is the prediction map\n",
    "def bleuFromMaps(m1, m2):\n",
    "  score = [0] * 5\n",
    "  num = 0.0\n",
    "\n",
    "  for key in m1:\n",
    "    if key in m2:\n",
    "      bl = bleu(m1[key], m2[key][0])\n",
    "      score = [ score[i] + bl[i] for i in range(0, len(bl))]\n",
    "      num += 1\n",
    "  return [s * 100.0 / num for s in score]\n",
    "\n",
    "\"\"\"if __name__ == '__main__':\n",
    "  reference_file = sys.argv[1]\n",
    "  predictions = []\n",
    "  for row in sys.stdin:\n",
    "    predictions.append(row)\"\"\"\n",
    "(goldMap, predictionMap) = computeMaps(preds5, reference) \n",
    "print(bleuFromMaps(goldMap, predictionMap)[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8_JC_WIfvRCG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "8OVfBrGVjezr",
    "RtC425vH0XEz",
    "m-vs8TovMnDt",
    "ODtz4A-YkssU"
   ],
   "machine_shape": "hm",
   "name": "finalEnsembleModel.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
